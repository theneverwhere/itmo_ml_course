{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoDgY8LQLuhv"
   },
   "source": [
    "## Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwo3ddTS-SL3"
   },
   "source": [
    "Реализовать класс для работы с линейной регрессией"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d0aa0",
   "metadata": {},
   "source": [
    "### План работы\n",
    "\n",
    "- Реализуем собственный класс линейной регрессии с опциональной L1/L2 регуляризацией и тремя способами расчёта весов.\n",
    "- Подготовим датасет автомобилей: закодируем категории, разделим на обучение/тест.\n",
    "- Сравним конфигурации модели (матрица, GD, SGD) между собой и с `sklearn`, замерим качество и скорость.\n",
    "- Посмотрим, какие признаки вносят наибольший вклад."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e91139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class MyLinearRegression:\n",
    "    \"\"\"\n",
    "    Простая линейная регрессия с опциональной L1/L2 регуляризацией.\n",
    "    Поддерживает три способа оценки весов: матрично, полноразмерным градиентным спуском и SGD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regularization=None, weight_calc='matrix', lambda_1=None, lambda_2=None,\n",
    "                 batch_size=20, learning_rate=0.05, n_epochs=500, standardize=True, random_state=42):\n",
    "        if regularization not in [None, 'l1', 'l2', 'l1l2']:\n",
    "            raise TypeError(f\"Параметр regularization не может принимать значение '{regularization}'\")\n",
    "        if weight_calc not in ['matrix', 'gd', 'sgd']:\n",
    "            raise TypeError(f\"Параметр weight_calc не может принимать значение '{weight_calc}'\")\n",
    "        if regularization in ['l1', 'l1l2'] and lambda_1 is None:\n",
    "            raise TypeError(\"Значение коэффициента регуляризации l1 не задано\")\n",
    "        if regularization in ['l2', 'l1l2'] and lambda_2 is None:\n",
    "            raise TypeError(\"Значение коэффициента регуляризации l2 не задано\")\n",
    "\n",
    "        self.regularization = regularization\n",
    "        self.weight_calc = weight_calc\n",
    "        self.lambda_1 = lambda_1 if lambda_1 is not None else 0.0\n",
    "        self.lambda_2 = lambda_2 if lambda_2 is not None else 0.0\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.standardize = standardize\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.feature_mean_ = None\n",
    "        self.feature_std_ = None\n",
    "        self.weights_ = None\n",
    "        self.coefs_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def _standardize(self, X, fit=False):\n",
    "        X_arr = np.array(X, dtype=float)\n",
    "        if self.standardize:\n",
    "            if fit:\n",
    "                self.feature_mean_ = X_arr.mean(axis=0)\n",
    "                self.feature_std_ = X_arr.std(axis=0)\n",
    "                # Чтобы не делить на ноль, нулевые std заменяем на 1\n",
    "                self.feature_std_[self.feature_std_ == 0] = 1.0\n",
    "            X_arr = (X_arr - self.feature_mean_) / self.feature_std_\n",
    "        else:\n",
    "            if fit and self.feature_mean_ is None:\n",
    "                self.feature_mean_ = np.zeros(X_arr.shape[1])\n",
    "                self.feature_std_ = np.ones(X_arr.shape[1])\n",
    "        return X_arr\n",
    "\n",
    "    def _add_bias(self, X_arr):\n",
    "        ones = np.ones((X_arr.shape[0], 1))\n",
    "        return np.hstack([ones, X_arr])\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        X_scaled = self._standardize(X, fit=True)\n",
    "        y_arr = np.array(y, dtype=float).reshape(-1, 1)\n",
    "        X_design = self._add_bias(X_scaled)\n",
    "\n",
    "        if self.weight_calc == 'matrix':\n",
    "            reg_matrix = np.zeros((X_design.shape[1], X_design.shape[1]))\n",
    "            if self.regularization == 'l2':\n",
    "                reg_matrix = self.lambda_2 * np.eye(X_design.shape[1])\n",
    "                reg_matrix[0, 0] = 0.0  # не штрафуем сдвиг\n",
    "            XtX = X_design.T @ X_design + reg_matrix\n",
    "            XtY = X_design.T @ y_arr\n",
    "            self.weights_ = np.linalg.pinv(XtX) @ XtY\n",
    "        else:\n",
    "            rng = np.random.default_rng(self.random_state)\n",
    "            weights = np.zeros((X_design.shape[1], 1))\n",
    "            for _ in range(self.n_epochs):\n",
    "                if self.weight_calc == 'sgd':\n",
    "                    idx = rng.permutation(X_design.shape[0])\n",
    "                    X_shuffled = X_design[idx]\n",
    "                    y_shuffled = y_arr[idx]\n",
    "                    batches = [\n",
    "                        (X_shuffled[i:i + self.batch_size], y_shuffled[i:i + self.batch_size])\n",
    "                        for i in range(0, len(X_shuffled), self.batch_size)\n",
    "                    ]\n",
    "                else:  # полный градиент на всём датасете\n",
    "                    batches = [(X_design, y_arr)]\n",
    "\n",
    "                for X_batch, y_batch in batches:\n",
    "                    preds = X_batch @ weights\n",
    "                    errors = preds - y_batch\n",
    "                    grad = (2 / len(X_batch)) * (X_batch.T @ errors)\n",
    "\n",
    "                    # Добавляем регуляризацию только к коэффициентам (не к сдвигу)\n",
    "                    if self.regularization in ['l2', 'l1l2']:\n",
    "                        grad[1:] += 2 * self.lambda_2 * weights[1:]\n",
    "                    if self.regularization in ['l1', 'l1l2']:\n",
    "                        grad[1:] += self.lambda_1 * np.sign(weights[1:])\n",
    "\n",
    "                    weights -= self.learning_rate * grad\n",
    "            self.weights_ = weights\n",
    "\n",
    "        self.intercept_ = float(self.weights_[0])\n",
    "        self.coefs_ = self.weights_[1:].ravel()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.array, ss=True):\n",
    "        if self.weights_ is None:\n",
    "            raise RuntimeError('Сначала вызовите fit, чтобы обучить модель.')\n",
    "        X_arr = np.array(X, dtype=float)\n",
    "        if self.standardize and ss:\n",
    "            X_arr = (X_arr - self.feature_mean_) / self.feature_std_\n",
    "        X_design = self._add_bias(X_arr)\n",
    "        return (X_design @ self.weights_).ravel()\n",
    "\n",
    "    def score(self, X: np.array, y: np.array):\n",
    "        y_true = np.array(y, dtype=float).ravel()\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n",
    "        return 1 - ss_res / ss_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eadc0f",
   "metadata": {},
   "source": [
    "### Подготовка данных для эксперимента\n",
    "\n",
    "Используем датасет цен на Fiat 500. Числовые признаки оставляем как есть, категориальные (`model`, `transmission`) кодируем в one-hot. Затем делим данные на обучающую и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Загружаем и кодируем категории\n",
    "data_path = Path('Lab-4') / 'Used_fiat_500_in_Italy_dataset.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "data = pd.get_dummies(df, columns=['model', 'transmission'], drop_first=True)\n",
    "\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Для sklearn готовим отмасштабированную версию (наша модель масштабирует сама)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Размер выборок:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd4f2c",
   "metadata": {},
   "source": [
    "### Сравнение моделей\n",
    "\n",
    "Собираем несколько конфигураций `MyLinearRegression` и сравниваем их с библиотечными `LinearRegression` и `Ridge`. Метрики: $R^2$ (чем ближе к 1, тем лучше) и RMSE. Также меряем время обучения и предсказания в миллисекундах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9dbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te, name):\n",
    "    start_fit = time.perf_counter()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    train_time = (time.perf_counter() - start_fit) * 1000\n",
    "\n",
    "    start_pred = time.perf_counter()\n",
    "    preds = model.predict(X_te)\n",
    "    pred_time = (time.perf_counter() - start_pred) * 1000\n",
    "\n",
    "    r2 = r2_score(y_te, preds)\n",
    "    mse = mean_squared_error(y_te, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {\n",
    "        'model': name,\n",
    "        'r2': round(r2, 3),\n",
    "        'rmse': round(rmse, 1),\n",
    "        'train_ms': round(train_time, 1),\n",
    "        'predict_ms': round(pred_time, 3)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "configs = [\n",
    "    ('my_matrix', MyLinearRegression(weight_calc='matrix', regularization=None, standardize=True)),\n",
    "    ('my_ridge_matrix', MyLinearRegression(weight_calc='matrix', regularization='l2', lambda_2=0.5, standardize=True)),\n",
    "    ('my_gd_l2', MyLinearRegression(weight_calc='gd', regularization='l2', lambda_2=0.1, learning_rate=0.05, n_epochs=800, standardize=True)),\n",
    "    ('my_sgd_l1', MyLinearRegression(weight_calc='sgd', regularization='l1', lambda_1=0.001, learning_rate=0.05, n_epochs=600, batch_size=32, standardize=True)),\n",
    "]\n",
    "\n",
    "for name, model in configs:\n",
    "    res = evaluate_model(model, X_train, y_train, X_test, y_test, name)\n",
    "    results.append(res)\n",
    "    trained_models[name] = model\n",
    "\n",
    "# Библиотечные модели (используем заранее масштабированные признаки)\n",
    "sklearn_lr = LinearRegression()\n",
    "results.append(evaluate_model(sklearn_lr, X_train_scaled, y_train, X_test_scaled, y_test, 'sklearn_lr'))\n",
    "\n",
    "sklearn_ridge = Ridge(alpha=0.5)\n",
    "results.append(evaluate_model(sklearn_ridge, X_train_scaled, y_train, X_test_scaled, y_test, 'sklearn_ridge'))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print('Сводная таблица:')\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc80bf",
   "metadata": {},
   "source": [
    "### Важность признаков\n",
    "\n",
    "Берём лучшую из собственных моделей по RMSE и смотрим, какие признаки дают самый большой по модулю коэффициент (признаки уже отмасштабированы внутри модели)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f57a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем лучшую пользовательскую модель\n",
    "custom_results = results_df[results_df['model'].str.startswith('my_')].sort_values('rmse')\n",
    "best_name = custom_results.iloc[0]['model']\n",
    "best_model = trained_models[best_name]\n",
    "\n",
    "coef_table = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coef': best_model.coefs_\n",
    "})\n",
    "coef_table['abs_coef'] = coef_table['coef'].abs()\n",
    "\n",
    "print('Лучшая конфигурация:', best_name)\n",
    "print('Свободный член (intercept):', best_model.intercept_)\n",
    "print('Топ-10 признаков по модулю коэффициента:')\n",
    "print(coef_table.sort_values('abs_coef', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d786fc",
   "metadata": {},
   "source": [
    "**Выводы по задаче 1.**\n",
    "\n",
    "- Матричное решение даёт эталонную точность и обучается мгновенно, но не работает с L1.\n",
    "- L2-регуляризация чуть ухудшает $R^2$, но снижает риск переобучения и делает веса аккуратнее.\n",
    "- Полный GD и SGD дают сравнимое качество, но требуют настройки шага и числа эпох; на небольшом датасете время всё равно меньше миллисекунды.\n",
    "- `sklearn` показывает почти те же метрики, что подтверждает корректность реализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKO_QAb5Lmdd"
   },
   "source": [
    "## Задача 2\n",
    "\n",
    "[Соревнование на Kaggle](https://kaggle.com/competitions/yadro-regression-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaaad76",
   "metadata": {},
   "source": [
    "### Набросок для задачи 2 (Kaggle)\n",
    "\n",
    "- Скачать тренировочные и тестовые данные с Kaggle и объединить признаки для единой обработки.\n",
    "- Провести базовую очистку: обработать пропуски, закодировать категории (one-hot/таргет-кодирование), нормировать числовые столбцы.\n",
    "- Быстрый бейзлайн: `LinearRegression`, `Ridge/Lasso`, градиентный бустинг (`CatBoostRegressor` удобен с категориями).\n",
    "- Сделать кросс-валидацию по RMSE, зафиксировать лучший набор гиперпараметров, сформировать сабмит и загрузить на соревнование."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
